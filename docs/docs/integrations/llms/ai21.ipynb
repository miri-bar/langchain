{
 "cells": [
  {
   "cell_type": "raw",
   "id": "602a52a4",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: AI21 Labs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9597802c",
   "metadata": {},
   "source": [
    "# AI21LLM\n",
    "\n",
    "This example goes over how to use LangChain to interact with `AI21` Jurassic models. To use the Jamba model, use the [ChatAI21 object](https://python.langchain.com/v0.2/docs/integrations/chat/ai21/) instead.\n",
    "\n",
    "[See a full list of AI21 models and tools on LangChain.](https://pypi.org/project/langchain-ai21/)\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "id": "59c710c4",
   "metadata": {},
   "source": [
    "!pip install -qU langchain-ai21"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "560a2f9254963fd7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "We'll need to get a [AI21 API key](https://docs.ai21.com/) and set the `AI21_API_KEY` environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "id": "035dea0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"AI21_API_KEY\"] = getpass()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1891df96eb076e1a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "id": "98f70927a87e4745",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from langchain_ai21 import AI21LLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "model = AI21LLM(model=\"j2-ultra\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tool Calls / Function Calling",
   "id": "85dce82668f90a1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "AI21 models incorporate the Function Calling feature to support custom user functions.\n",
    "The models generate structured data that includes the function name and proposed arguments. This data empowers applications to call external APIs and incorporate the resulting information into subsequent model prompts, enriching responses with real-time data and context. Through function calling, users can access and utilize various services like transportation APIs and financial data providers to obtain more accurate and relevant answers.\n",
    "Here is an example of how to use function calling with AI21 models in LangChain:"
   ],
   "id": "8cd1248becd9eb6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_core.messages import HumanMessage, ToolMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_ai21.chat_models import ChatAI21\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "os.environ[\"AI21_API_KEY\"] = getpass()\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str, date: str) -> str:\n",
    "    \"\"\"“Provide the weather for the specified location on the given date.”\"\"\"\n",
    "    if location == \"New York\" and date == \"2024-12-05\":\n",
    "        return \"25 celsius\"\n",
    "    elif location == \"New York\" and date == \"2024-12-06\":\n",
    "        return \"27 celsius\"\n",
    "    elif location == \"London\" and date == \"2024-12-05\":\n",
    "        return \"22 celsius\"\n",
    "    return \"32 celsius\"\n",
    "\n",
    "llm = ChatAI21(model=\"jamba-1.5-mini\")\n",
    "\n",
    "llm_with_tools = llm.bind_tools([convert_to_openai_tool(get_weather)])\n",
    "\n",
    "chat_messages = [SystemMessage(content=\"You are a helpful assistant. You can use the provided tools \"\n",
    "                                       \"to assist with various tasks and provide accurate information\")]\n",
    "\n",
    "human_messages = [\n",
    "    HumanMessage(content=\"What is the forecast for the weather in New York on December 5, 2024?\"),\n",
    "    HumanMessage(content=\"And what about the 2024-12-06?\"),\n",
    "    HumanMessage(content=\"OK, thank you.\"),\n",
    "    HumanMessage(content=\"What is the expected weather in London on December 5, 2024?\")]\n",
    "\n",
    "\n",
    "for human_message in human_messages:\n",
    "    print(f\"User: {human_message.content}\")\n",
    "    chat_messages.append(human_message)\n",
    "    response = llm_with_tools.invoke(chat_messages)\n",
    "    chat_messages.append(response)\n",
    "    if response.tool_calls:\n",
    "        tool_call = response.tool_calls[0]\n",
    "        if tool_call[\"name\"] == \"get_weather\":\n",
    "            weather = get_weather.invoke(\n",
    "                {\"location\": tool_call[\"args\"][\"location\"], \"date\": tool_call[\"args\"][\"date\"]})\n",
    "            chat_messages.append(ToolMessage(content=weather, tool_call_id=tool_call[\"id\"]))\n",
    "            llm_answer = llm_with_tools.invoke(chat_messages)\n",
    "            print(f\"Assistant: {llm_answer.content}\")\n",
    "    else:\n",
    "        print(f\"Assistant: {response.content}\")\n"
   ],
   "id": "16619ababeb30431",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e5c0af1bed041b3d"
  },
  {
   "cell_type": "markdown",
   "id": "9965c10269159ed1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# AI21 Contextual Answer\n",
    "\n",
    "You can use AI21's contextual answers model to receives text or document, serving as a context,\n",
    "and a question and returns an answer based entirely on this context.\n",
    "\n",
    "This means that if the answer to your question is not in the document,\n",
    "the model will indicate it (instead of providing a false answer)"
   ]
  },
  {
   "cell_type": "code",
   "id": "411adf42eab80829",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from langchain_ai21 import AI21ContextualAnswers\n",
    "\n",
    "tsm = AI21ContextualAnswers()\n",
    "\n",
    "response = tsm.invoke(input={\"context\": \"Your context\", \"question\": \"Your question\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "af59ffdbf4964875",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can also use it with chains and output parsers and vector DBs"
   ]
  },
  {
   "cell_type": "code",
   "id": "bc63830f921b4ac9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from langchain_ai21 import AI21ContextualAnswers\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "tsm = AI21ContextualAnswers()\n",
    "chain = tsm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\"context\": \"Your context\", \"question\": \"Your question\"},\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e971737741ff4ec9aff7dc6155a1060a59a8a6d52c757dbbe66bf8ee389494b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
